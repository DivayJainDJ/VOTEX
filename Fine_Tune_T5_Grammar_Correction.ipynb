{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune T5 for Grammar Correction (STT Errors)\n",
        "\n",
        "This notebook fine-tunes a T5 model specifically for correcting grammar errors from speech-to-text transcriptions.\n",
        "\n",
        "**Steps:**\n",
        "1. Install dependencies\n",
        "2. Create/load dataset of incorrectâ†’correct pairs\n",
        "3. Fine-tune T5-small model\n",
        "4. Test the model\n",
        "5. Export for use in your application"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate sentencepiece -q"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Training Dataset\n",
        "\n",
        "Create a dataset of common STT grammar errors and their corrections."
      ],
      "metadata": {
        "id": "dataset"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Sample dataset - expand this with more examples!\n",
        "training_data = [\n",
        "    # Word order errors\n",
        "    {\"incorrect\": \"i have a tomorrow match\", \"correct\": \"i have a match tomorrow\"},\n",
        "    {\"incorrect\": \"i have a today meeting\", \"correct\": \"i have a meeting today\"},\n",
        "    {\"incorrect\": \"i have a tonight party\", \"correct\": \"i have a party tonight\"},\n",
        "    {\"incorrect\": \"we have a yesterday game\", \"correct\": \"we had a game yesterday\"},\n",
        "    \n",
        "    # Article errors\n",
        "    {\"incorrect\": \"this is a umbrella\", \"correct\": \"this is an umbrella\"},\n",
        "    {\"incorrect\": \"this is an laptop\", \"correct\": \"this is a laptop\"},\n",
        "    {\"incorrect\": \"i have a apple\", \"correct\": \"i have an apple\"},\n",
        "    {\"incorrect\": \"she is an teacher\", \"correct\": \"she is a teacher\"},\n",
        "    \n",
        "    # Subject-verb agreement\n",
        "    {\"incorrect\": \"he dont like it\", \"correct\": \"he doesn't like it\"},\n",
        "    {\"incorrect\": \"she have a car\", \"correct\": \"she has a car\"},\n",
        "    {\"incorrect\": \"they was going\", \"correct\": \"they were going\"},\n",
        "    {\"incorrect\": \"he do his homework\", \"correct\": \"he does his homework\"},\n",
        "    \n",
        "    # Common mistakes\n",
        "    {\"incorrect\": \"i could of done it\", \"correct\": \"i could have done it\"},\n",
        "    {\"incorrect\": \"i should of known\", \"correct\": \"i should have known\"},\n",
        "    {\"incorrect\": \"its a nice day\", \"correct\": \"it's a nice day\"},\n",
        "    {\"incorrect\": \"your going home\", \"correct\": \"you're going home\"},\n",
        "    \n",
        "    # Tense errors\n",
        "    {\"incorrect\": \"i go yesterday\", \"correct\": \"i went yesterday\"},\n",
        "    {\"incorrect\": \"she see me tomorrow\", \"correct\": \"she will see me tomorrow\"},\n",
        "    {\"incorrect\": \"we was there\", \"correct\": \"we were there\"},\n",
        "    \n",
        "    # Preposition errors\n",
        "    {\"incorrect\": \"different than you\", \"correct\": \"different from you\"},\n",
        "    {\"incorrect\": \"married with her\", \"correct\": \"married to her\"},\n",
        "    {\"incorrect\": \"in the weekend\", \"correct\": \"on the weekend\"},\n",
        "    \n",
        "    # Add more examples here...\n",
        "    {\"incorrect\": \"i dont have no money\", \"correct\": \"i don't have any money\"},\n",
        "    {\"incorrect\": \"he go always to school\", \"correct\": \"he always goes to school\"},\n",
        "    {\"incorrect\": \"she is more better\", \"correct\": \"she is better\"},\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(training_data)\n",
        "\n",
        "# Split into train/validation (80/20)\n",
        "train_size = int(0.8 * len(df))\n",
        "train_df = df[:train_size]\n",
        "val_df = df[train_size:]\n",
        "\n",
        "# Create Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset\n",
        "})\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")\n",
        "print(\"\\nSample:\")\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "create_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare Data for T5"
      ],
      "metadata": {
        "id": "prepare"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load T5 tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Add prefix for T5\n",
        "    inputs = [\"grammar: \" + text for text in examples[\"incorrect\"]]\n",
        "    targets = examples[\"correct\"]\n",
        "    \n",
        "    # Tokenize\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "print(\"Dataset tokenized successfully!\")"
      ],
      "metadata": {
        "id": "tokenize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Fine-tune T5 Model"
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import numpy as np\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./grammar-correction-t5\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,  # Use mixed precision for faster training\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Metric for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Simple accuracy metric\n",
        "    correct = sum([pred.strip() == label.strip() for pred, label in zip(decoded_preds, decoded_labels)])\n",
        "    return {\"accuracy\": correct / len(decoded_labels)}\n",
        "\n",
        "# Create trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "train_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Test the Model"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_grammar(text):\n",
        "    \"\"\"Test the fine-tuned model\"\"\"\n",
        "    input_text = \"grammar: \" + text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    outputs = model.generate(**inputs, max_length=128)\n",
        "    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    return corrected\n",
        "\n",
        "# Test examples\n",
        "test_sentences = [\n",
        "    \"i have a tomorrow match\",\n",
        "    \"this is an laptop\",\n",
        "    \"he dont like it\",\n",
        "    \"i could of done it\",\n",
        "    \"she have a car\",\n",
        "]\n",
        "\n",
        "print(\"Testing fine-tuned model:\\n\")\n",
        "for sentence in test_sentences:\n",
        "    corrected = correct_grammar(sentence)\n",
        "    print(f\"Input:     {sentence}\")\n",
        "    print(f\"Corrected: {corrected}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Save and Export Model"
      ],
      "metadata": {
        "id": "save"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model locally\n",
        "model.save_pretrained(\"./grammar-correction-t5-final\")\n",
        "tokenizer.save_pretrained(\"./grammar-correction-t5-final\")\n",
        "\n",
        "print(\"Model saved to ./grammar-correction-t5-final\")\n",
        "\n",
        "# Download to your computer\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create zip file\n",
        "shutil.make_archive('grammar-correction-model', 'zip', './grammar-correction-t5-final')\n",
        "files.download('grammar-correction-model.zip')\n",
        "\n",
        "print(\"\\nModel downloaded! Extract and use in your application.\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. (Optional) Push to Hugging Face Hub"
      ],
      "metadata": {
        "id": "hub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run if you want to upload to Hugging Face\n",
        "\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "\n",
        "# model.push_to_hub(\"your-username/grammar-correction-stt\")\n",
        "# tokenizer.push_to_hub(\"your-username/grammar-correction-stt\")\n",
        "\n",
        "# print(\"Model uploaded to Hugging Face Hub!\")"
      ],
      "metadata": {
        "id": "push_hub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. How to Use in Your Application\n",
        "\n",
        "After downloading the model, use it in your Python code:\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load your fine-tuned model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./grammar-correction-t5-final\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./grammar-correction-t5-final\")\n",
        "\n",
        "def correct_grammar(text):\n",
        "    input_text = \"grammar: \" + text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=128)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Use it\n",
        "result = correct_grammar(\"i have a tomorrow match\")\n",
        "print(result)  # Output: \"i have a match tomorrow\"\n",
        "```"
      ],
      "metadata": {
        "id": "usage"
      }
    }
  ]
}
